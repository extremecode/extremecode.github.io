#Logistic Regression

![image](https://user-images.githubusercontent.com/20191454/162799546-29a1fb3c-7f43-430a-993c-ed054b12adc3.png)
![image](https://user-images.githubusercontent.com/20191454/162863453-9ddb632d-c7a4-4ae7-89af-49bd778f1bef.png)
![image](https://user-images.githubusercontent.com/20191454/162863519-5f4f2fa5-67e5-4a4d-ac44-00cdd9170d60.png)
![image](https://user-images.githubusercontent.com/20191454/162864066-e89ef933-e809-4daa-997c-37d305113b00.png)

Use the test dataset to check the model generalizability, if you tune the model based on the test dataset the results achieved will be questionable. 

You can handle the variability in a data set caused by random chance in these two ways:

Collect more unseen data for testing

Cross-validate the training data

Cross-validation is a popular technique used in ensemble models. You will learn about cross-validation later in the course. 

You learned how to build a supervised machine learning model. 

We discussed the principle of generalizability.

Next, you briefly learned about overfitting and underfitting.

Further, you understood how splitting a data set into training, validation, and test data sets can be beneficial when evaluating a model’s performance.

You learned about the limitations of splitting a data set.

 ```markdown
 Before you learn these metrics, you need to understand an important concept: positive and negative classes. All the examples of classification models we have discussed so far had only two classes. For instance, in the case of credit approval, the classes were 'Approve credit' or 'Not approve credit.' In the case of the tumor detection model, the classes were 'cancerous tumor' or 'non-cancerous tumor.' Classification models with two classes are called binary class classification models. Later in this module, we will discuss examples with multiple classes in output. 
 ```
 
 ![image](https://user-images.githubusercontent.com/20191454/162868974-382b6c78-ee4d-4f19-8341-578d3375239b.png)
![image](https://user-images.githubusercontent.com/20191454/162870014-723bac77-4871-4e25-a197-b30e5b86be90.png)
![image](https://user-images.githubusercontent.com/20191454/162870093-4e77db37-8ea1-4f31-aa8d-d17c460a9cb8.png)
![image](https://user-images.githubusercontent.com/20191454/162870464-0334859c-c3d4-4595-88f2-fe76a5032f4a.png)
![image](https://user-images.githubusercontent.com/20191454/162871221-c7a11fd2-2509-438e-b6b5-95a0fa51f286.png)
![image](https://user-images.githubusercontent.com/20191454/162871281-5b46df64-6094-4d84-9985-f517172a2c6b.png)
https://towardsdatascience.com/confusion-matrix-for-your-multi-class-machine-learning-model-ff9aa3bf7826



 
![image](https://user-images.githubusercontent.com/20191454/162979947-3eaba18a-776b-4b01-ba30-397016cb11a5.png)
![image](https://user-images.githubusercontent.com/20191454/162981000-23d7316a-48e1-4f25-9cc7-db8f036b264c.png)

![image](https://user-images.githubusercontent.com/20191454/162981415-17ff51f0-3d0a-4588-8f1a-9143e189319e.png)
![image](https://user-images.githubusercontent.com/20191454/162984209-98c8efe5-454e-4e1d-91da-7efbb5507808.png)
![image](https://user-images.githubusercontent.com/20191454/163661274-bacf1275-ae72-4576-aff1-831ac35c5510.png)
![image](https://user-images.githubusercontent.com/20191454/163661381-990d6046-9e19-4c7e-8884-78fdbc5c25fd.png)
https://medium.com/@maithilijoshi6/a-comparison-between-linear-and-logistic-regression-8aea40867e2d
![image](https://user-images.githubusercontent.com/20191454/163661400-f7ad4334-5bb6-4a0b-a376-32d33e9aaf88.png)
![image](https://user-images.githubusercontent.com/20191454/163661443-102a4b80-213b-4958-8c26-5ab39b995b48.png)
![image](https://user-images.githubusercontent.com/20191454/163661981-6c3d0ea5-d0e9-4b90-898f-4c7182bb3ea1.png)
![image](https://user-images.githubusercontent.com/20191454/163661988-9840e2be-4c58-4643-8af2-d3bf571e811d.png)
![image](https://user-images.githubusercontent.com/20191454/163661993-72967e48-43bb-40ab-ac8f-36b4e80cc049.png)
![image](https://user-images.githubusercontent.com/20191454/163667908-95998f97-b895-446d-8047-3080b1f97438.png)
![image](https://user-images.githubusercontent.com/20191454/163667924-ebbb85a3-1687-4e0f-92a5-4bfbf08c0578.png)
![image](https://user-images.githubusercontent.com/20191454/163668896-e6e428d6-a02a-46c2-a292-cf081c311fa8.png)
![image](https://user-images.githubusercontent.com/20191454/163668903-920a0e82-c85c-4fb1-b007-6b3b0a9edf67.png)
![image](https://user-images.githubusercontent.com/20191454/163725968-572cda27-57c0-4bad-b1c5-b610e0b0647a.png)
![image](https://user-images.githubusercontent.com/20191454/163725981-d8c5c7ca-6f4a-4f2d-a537-23d24c844c89.png)
![image](https://user-images.githubusercontent.com/20191454/163726051-f3749ffc-f628-4c9b-bdf5-b7451eaf9d5e.png)
![image](https://user-images.githubusercontent.com/20191454/163726096-5b6ee434-1278-4136-a12c-9a9182fab0e4.png)
![image](https://user-images.githubusercontent.com/20191454/163726100-7b711234-01db-41dc-8dbe-585683bdbec3.png)
![image](https://user-images.githubusercontent.com/20191454/163726113-181f335b-8e6f-47d1-b77d-39de9e9a816c.png)
![image](https://user-images.githubusercontent.com/20191454/163726308-65bc789b-5cbd-4e45-865f-61df8c9c1cb8.png)
![image](https://user-images.githubusercontent.com/20191454/163726318-ec58b010-d4d8-4322-878e-327439700a53.png)
![image](https://user-images.githubusercontent.com/20191454/163726390-2d433eb1-8f91-41b0-aa62-54f74fe52f89.png)
![image](https://user-images.githubusercontent.com/20191454/163726497-cba9c6b5-f388-4b75-8a4b-e7991366e61b.png)
![image](https://user-images.githubusercontent.com/20191454/163726566-41bf710b-40a8-4c7e-b7b5-37622ad08c55.png)
![image](https://user-images.githubusercontent.com/20191454/163726573-2471ae2e-df9e-4572-96c5-441c91fe8b94.png)




![image](https://user-images.githubusercontent.com/20191454/163726787-077d6478-dd47-4027-a6ce-61bfc9564c00.png)
![image](https://user-images.githubusercontent.com/20191454/163726806-b998c0c1-a3f9-4a6f-9ee9-c76b8c52403b.png)
![image](https://user-images.githubusercontent.com/20191454/163726825-184c2371-18ed-4ffa-af69-30fe53353cd4.png)
![image](https://user-images.githubusercontent.com/20191454/163726837-44f19ad4-a384-4ce0-919b-1cf8fa4c1060.png)
![image](https://user-images.githubusercontent.com/20191454/163726908-cd2ed6ce-9322-4508-b970-cba6350fe267.png)
![image](https://user-images.githubusercontent.com/20191454/163726925-1bf5e53d-8cc7-4c4b-833d-6f81cbb7f341.png)
![image](https://user-images.githubusercontent.com/20191454/163726931-de7932df-87ed-4a26-bdb7-357db6ecf7d6.png)
![image](https://user-images.githubusercontent.com/20191454/163726940-46b75389-4f4c-496e-b6b9-5a110dc14d55.png)
![image](https://user-images.githubusercontent.com/20191454/163726951-33a632d3-fd2b-4918-b589-d0aae6173121.png)
![image](https://user-images.githubusercontent.com/20191454/163727151-4a382c1a-9b18-44fe-9846-8271c3dd24c6.png)
![image](https://user-images.githubusercontent.com/20191454/163727160-dbc21c86-0a5d-4f9b-9a41-6f2bbb5c2733.png)
![image](https://user-images.githubusercontent.com/20191454/163727203-82562564-28a3-499c-ad68-5b48df663872.png)
![image](https://user-images.githubusercontent.com/20191454/163727209-048a71c7-146b-4742-ad56-b2938adb8cac.png)
https://www.geeksforgeeks.org/ml-stochastic-gradient-descent-sgd/
![image](https://user-images.githubusercontent.com/20191454/163727240-8bb19ed4-21c4-4462-832e-0944faab26b2.png)
https://ml-cheatsheet.readthedocs.io/en/latest/gradient_descent.html
![image](https://user-images.githubusercontent.com/20191454/163727257-f799ee3e-406a-4c47-9ee9-096426d9d829.png)
![image](https://user-images.githubusercontent.com/20191454/163727363-f043d56a-0995-4194-bc29-ef4958146ea1.png)
![image](https://user-images.githubusercontent.com/20191454/163727593-bf6ad01b-3bc9-45ff-ad9a-6090c99f1b02.png)




```markdown
ns: 

All the mathematical tools and concepts needed to solve a classification problem

The use of these tools to build an algorithm—logistic model—to solve the problem 

Let’s summarize the topics covered in each sub-session.

Mathematical tools and concepts 

Sigmoid function: This is best suited for converting the continuous output of the wTxi term to probabilities. This is the probability of getting a positive class, given the features of the data. 

Odds and log odds: This helps interpret the relationship between the change in the inputs to the change in the probability of the positive class.

Gradient descent algorithm: This algorithm helps minimize the log loss cost function. The negative sign of the gradient of the log loss function gives the direction in which the minima will be found, and its magnitude gives information about how big a step you can take in an iteration.

 Logistic model building

Likelihood function: This gives the likelihood of the guessed labels being equal to the actual labels. It is calculated by combining the probabilities of the Bernoulli process with the sigmoid of the product of features and weights.  

Maximum likelihood estimation: Taking the logarithm and simplifying the likelihood function will give the log-likelihood function. The log-likelihood function gives the probability of the variable y, pi being equal to the actual variable yi, given the parameters of the model—weights and bias/intercept. To fit the assumed parameter to the given data, the log-likelihood needs to be maximized. This process is called maximum likelihood estimation. 

Minimizing the log loss function: In machine learning, it is preferred to minimize a function. So the sign of the maximum likelihood function is flipped to get the log loss function that can be minimized. The sigmoid of 
β
T
b
i
 is substituted into the log loss function, and then it is minimized. 

Gradient optimization for the log loss function: The gradient descent algorithm is used to find the weights that will give the minimum loss. Using the weights after convergence and the sigmoid function, the predictions are made for the unseen data points. 

​
```





![image](https://user-images.githubusercontent.com/20191454/163727813-72d29225-764b-4169-bb84-5d59dea9ebdf.png)

















